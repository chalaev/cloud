* Errors
** (easily fixable?) gpg-files are not auto-clouded when opened
** 05/10 create-save-delete: file remains
Create a file in emacs, save, then erase it. It will be created on other hosts because action (erase) comes before the file
is downloaded from the cloud.
** 04/29 (occurs when adding/clouding multiple files) member: Wrong type argument: arrayp, nil
The problem is that it is not reproducible...

M-x cloud-add ~/learn/html

** 02/19 check if directories can sometimes be clouded
** 02/01 cloud-add allows unexisting files
(might be fixed on 02/01 by adding =-k= option to =make=)
=cloud-add= may add a file that will not exist at the time of subsequent =cloud-sync= 

** 01/21 Makefile is growing
(might be fixed on 01/27)
1. it is not cleaned after =cloud-sync=
2. the problem disappears after emacs is reloaded

** 01/21 cloud-forget does not cancel previously scheduled upload
might be fixed on 01/27

** 01/20 same files downloaded multiple times
- again and again, until they are not locally updated (and thus become younger than the remote ones).
Note that there is no similar problem with /upload/.

** 01/20 remote delete
does not work for /clouded/ files.
Deleted (on host A) file creates an action to be executed on host B,
but on host B the file is uploaded instead of deletion.
** .gz files
are uploaded, then clouded on another host, but are not downloaded there
write test!

** ghost actions
On one host I =cloud-forget= a file which was not clouded on another host (kalinin)
So there was nothing to do on kalinin, but the action did not disappear, it multiplicated itself instead, so
#+BEGIN_SRC emacs-lisp :results drawer
(mapcar #'format-action remote-actions)
#+END_SRC

#+RESULTS:
:RESULTS:
("2020-11-30 17:29:32 EST" 0 1 "/etc/backup2l.conf"  "kolmogorov"  
 "2020-11-30 17:29:32 EST" 0 1 "/etc/backup2l.conf"  "kolmogorov"  
 "2020-11-30 17:29:32 EST" 0 1 "/etc/backup2l.conf"  "kolmogorov" )
:END:

* To do
1. Write tests for renaming or deleting files.
2. Probably I should use =start-process= or =call-process= instead of =shell-command= because =start-process= is claimed to be more versatile for elisp than
   (interactive) =shell-command= or =async-shell-command=. On the other hand, I had problems when using =call-process= for =gpg= encoding, see [[file:learn.org][learn.org]].
3. Files should be unclouded after, say, 30 days of being not updated. For such files, regular daily backup is enough.
4. Tired project
5. Garbage cleaning: some of the files in the remote directory are not referenced in =~/.emacs.d/cloud/`hostname`/all=;
   Such "abandoned" files were created during debugging; they should be identified and erased. Similarly, some of the lines in =~/.emacs.d/cloud/individual.passes= are unused
   and should be removed.
     
* Ideas on future development

~JPEGs~ and ~PNGs~ are encrypted with AES-algorithm that [[https://imagemagick.org/script/cipher.php][may become vulnerable]] if the same password is used for multiple images; this is why every image gets an individual password.

I did not expect this project to grow that much;
some of the desired functions are still not implemented or half-implemented.

** Replace clouds with torrents? 
[[http://lftp.yar.ru][lftp]] supports torrents.

I will think of using torrents instead of clouds or using them together.

(At least when mounted using ~WebDav~) clouds are probably *even slowlier* than torrents for large files:
for example, it took me 13+ hours (!) to copy 403-megabyte archive to [[https://disk.yandex.com/][Yandex Disk]].
(This is not an exception – I am used to the fact that ~WebDav~-clouds are very slow.)

I did not use torrents for looong time but I think they we faster even 10 years ago.

Advantages of torrents:
1. With torrents we can forget about space limitation we always have for clouds, and
2. The more people use torrents, the harder it is to enforce [[https://www.fsf.org/search?SearchableText=DMCA][DMCA]]; probably 50 million of torrent users in the US
   would be enough to make it meaningless and powerless piece of paper.

Recently there was also a [[https://www.youtube.com/watch?v=AD9kEESRfg0][video]] about [[https://freenetproject.org/pages/documentation.html][freenet]] which is probably even better than torrents.

** Permutate image blocks
For every clouded image file, we create a sub-directory in the remote directory.
This sub-directory will contain NxM small pieces (icons) of the image, where N,M≥12.
Most of these pieces will be equally sized rectangulars, combined together into the inner part of the image.
Others will be parts of the thin frame.
On the one hand, N and M can not be to small; on the other hand each inner rectangular image should not be too small
– at least 50x40 pixels – so that JPEG (or another) image compression algorithm remains efficient.
This kind of protection is probably ok to protect innocent photos from face recognition robots.
And in case someone writes an algorithm trying to guess the correct icons' position, 
creating obstacles for it (so that decryption of a single photo would take at least an hour) seems easy.

** Other ideas
1. ~convert~ runs without parameters (e.g., controlling jpeg quality) for now. Thus, for example,
   a low-quality ~JPEG~ file may be grow about 3 times larger after it was encrypted and then decrypted back.
